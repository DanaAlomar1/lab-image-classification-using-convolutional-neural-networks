{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4rCKcndPybL"
      },
      "source": [
        "# Lab : Image Classification using Convolutional Neural Networks\n",
        "\n",
        "At the end of this laboratory, you would get familiarized with\n",
        "\n",
        "*   Creating deep networks using Keras\n",
        "*   Steps necessary in training a neural network\n",
        "*   Prediction and performance analysis using neural networks\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdglSzOi4Cp-"
      },
      "source": [
        "# **In case you use a colaboratory environment**\n",
        "By default, Colab notebooks run on CPU.\n",
        "You can switch your notebook to run with GPU.\n",
        "\n",
        "In order to obtain access to the GPU, you need to choose the tab Runtime and then select “Change runtime type” as shown in the following figure:\n",
        "\n",
        "![Changing runtime](https://miro.medium.com/max/747/1*euE7nGZ0uJQcgvkpgvkoQg.png)\n",
        "\n",
        "When a pop-up window appears select GPU. Ensure “Hardware accelerator” is set to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wkicuxZdrdq"
      },
      "source": [
        "# **Working with a new dataset: CIFAR-10**\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More information about CIFAR-10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "In Keras, the CIFAR-10 dataset is also preloaded in the form of four Numpy arrays. x_train and y_train contain the training set, while x_test and y_test contain the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9.\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "*   Visualize the images in CIFAR-10 dataset. Create a 10 x 10 plot showing 10 random samples from each class.\n",
        "*   Convert the labels to one-hot encoded form.\n",
        "*   Normalize the images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrb20KGMtTFq",
        "outputId": "1ff34086-11b0-43d0-fd0a-5ba344a82be3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TpOOYn6boWMc",
        "outputId": "7c5dbd53-1dc0-4c09-a989-ea25fc15779d"
      },
      "outputs": [],
      "source": [
        "# CIFAR-10 class labels\n",
        "class_labels = [\n",
        "    'Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "    'Dog', 'Frog', 'Horse', 'Ship', 'Truck'\n",
        "]\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
        "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Normalize images to the range [0,1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Visualize 10 random samples from each class in a 10x10 grid\n",
        "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
        "\n",
        "for class_idx in range(10):\n",
        "    class_images = x_train[y_train.flatten() == class_idx]\n",
        "    random_indices = np.random.choice(class_images.shape[0], 10, replace=False)\n",
        "\n",
        "    for i, img_idx in enumerate(random_indices):\n",
        "        ax = axes[class_idx, i]\n",
        "        ax.imshow(class_images[img_idx])\n",
        "        ax.axis('off')\n",
        "\n",
        "# Add class labels to the left side of the grid\n",
        "for ax, label in zip(axes[:, 0], class_labels):\n",
        "    ax.set_ylabel(label, rotation=90, size=10, labelpad=10)\n",
        "\n",
        "plt.suptitle(\"CIFAR-10 Sample Images (10 per Class)\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ER5WlMNRydp"
      },
      "source": [
        "## Define the following model (same as the one in tutorial)\n",
        "\n",
        "For the convolutional front-end, start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer.\n",
        "\n",
        "Use the input as (32,32,3).\n",
        "\n",
        "The filter maps can then be flattened to provide features to the classifier.\n",
        "\n",
        "Use a dense layer with 100 units before the classification layer (which is also a dense layer with softmax activation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfWCHxh8HGhN"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "iSN6riPISBMG",
        "outputId": "11440f7a-2f66-452f-d2d0-b5cbfdc229b8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Clear any previous models from memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),  # Convolutional layer\n",
        "    MaxPooling2D(pool_size=(2,2)),  # Pooling layer\n",
        "    Flatten(),  # Flattening the matrix into a vector\n",
        "    Dense(100, activation='relu'),  # Fully connected layer with 100 units\n",
        "    Dense(10, activation='softmax')  # Output layer with 10 classes\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtivbQJT39U"
      },
      "source": [
        " Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn8UzPBZugVp",
        "outputId": "0644e335-4ebe-4035-9f9a-87b943187a69"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Compile the model using loss function, optimizer, and evaluation metric\n",
        "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model on CIFAR-10 dataset\n",
        "history = model.fit(x_train, y_train_one_hot, epochs=50, batch_size=512, validation_data=(x_test, y_test_one_hot))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxZ8PcrqoWMh"
      },
      "source": [
        "  Plot the cross entropy loss curve and the accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "6dxBwGFToWMi",
        "outputId": "9bf0f7e5-c319-44b2-d3fd-1d19817510c0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Cross Entropy Loss Curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Cross Entropy Loss Curve')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy Curve\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mrWK5hSB_o"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## Defining Deeper Architectures: VGG Models\n",
        "\n",
        "*   Define a deeper model architecture for CIFAR-10 dataset and train the new model for 50 epochs with a batch size of 512. We will use VGG model as the architecture.\n",
        "\n",
        "Stack two convolutional layers with 32 filters, each of 3 x 3.\n",
        "\n",
        "Use a max pooling layer and next flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
        " >change the size of input to 64 x 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A80vLxW9FIek"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cgca5dUNSFNc",
        "outputId": "e78ac653-79fd-4e70-e78c-b89d2f82dd0a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Resizing\n",
        "\n",
        "# Clear any previous models from memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Define a VGG-like model for CIFAR-10 with input size 64x64\n",
        "model_vgg = Sequential([\n",
        "    # Resize input to 64x64\n",
        "    Resizing(64, 64),\n",
        "\n",
        "    # First two convolutional layers with 32 filters (3x3)\n",
        "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(64,64,3)),\n",
        "    Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "\n",
        "    # Max pooling layer\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Flatten the matrix into a vector\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully connected layer with 128 units\n",
        "    Dense(128, activation='relu'),\n",
        "\n",
        "    # Output layer with 10 classes\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_vgg.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_vgg = model_vgg.fit(x_train, y_train_one_hot, epochs=50, batch_size=512, validation_data=(x_test, y_test_one_hot))\n",
        "\n",
        "# Print the model summary\n",
        "model_vgg.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaPphEBUtlC"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc2qtU0mUvVA",
        "outputId": "b3a6ab3b-cf31-4c07-9abb-a3980cebbd33"
      },
      "outputs": [],
      "source": [
        "# Compile the model using categorical_crossentropy loss, SGD optimizer, and accuracy metric\n",
        "model_vgg.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on CIFAR-10 dataset for 50 epochs with a batch size of 512\n",
        "history_vgg = model_vgg.fit(x_train, y_train_one_hot, epochs=50, batch_size=512, validation_data=(x_test, y_test_one_hot))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2cRr2ZFSFds"
      },
      "source": [
        "* italicized text   Compare the performance of both the models by plotting the loss and accuracy curves of both the training steps. Does the deeper model perform better? Comment on the observation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "F8OSHAf5SJPr",
        "outputId": "aa9139a3-2ff4-4e6e-81e4-d9d8fd16ba86"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Loss Curves for both models\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Simple CNN - Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Simple CNN - Validation Loss')\n",
        "plt.plot(history_vgg.history['loss'], label='VGG Model - Train Loss', linestyle='dashed')\n",
        "plt.plot(history_vgg.history['val_loss'], label='VGG Model - Validation Loss', linestyle='dashed')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve Comparison')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy Curves for both models\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Simple CNN - Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Simple CNN - Validation Accuracy')\n",
        "plt.plot(history_vgg.history['accuracy'], label='VGG Model - Train Accuracy', linestyle='dashed')\n",
        "plt.plot(history_vgg.history['val_accuracy'], label='VGG Model - Validation Accuracy', linestyle='dashed')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Curve Comparison')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri9kU3wa3Rei"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "`# This is formatted as code`\n",
        "```\n",
        "\n",
        "**Comment on the observation**\n",
        "\n",
        "- VGG model learns faster and achieves higher training accuracy (~80%) but suffers from overfitting (unstable validation loss, lower validation accuracy ~60%).\n",
        "-  Simple CNN generalizes better but learns more slowly with lower accuracy (~60%).\n",
        "- To improve VGG, use data augmentation, dropout, and regularization to reduce overfitting.\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzXmO1WoSKMY"
      },
      "source": [
        "   Use predict function to predict the output for the test split\n",
        "*   Plot the confusion matrix for the new model and comment on the class confusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "DObaoxhaSMUg",
        "outputId": "75131e70-ffe8-401e-da72-c29c3c64df16"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Predict the output for the test set using the VGG model\n",
        "y_pred = model_vgg.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n",
        "y_true = np.argmax(y_test_one_hot, axis=1)  # True class labels\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for VGG Model on CIFAR-10')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBrvRomU5O_"
      },
      "source": [
        "**Comment here :**\n",
        "\n",
        "- Best accuracy for Airplanes, Ships, and Horses\n",
        "- High confusion between Birds & Cats, Dogs & Cats, and Automobiles & Trucks\n",
        "- Frogs & Deer have significant misclassification\n",
        "- Improvements: Use data augmentation, regularization, or fine-tuning for better accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffwVz-FLSNG7"
      },
      "source": [
        " Print the test accuracy for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4WX3_uLSN5I",
        "outputId": "41df029a-1bbd-4915-9c65-f0a5276e4086"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model_vgg.evaluate(x_test, y_test_one_hot)\n",
        "\n",
        "# Print test accuracy\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dySqfA6PVBjQ"
      },
      "source": [
        "## Define the complete VGG architecture.\n",
        "\n",
        "Stack two convolutional layers with 64 filters, each of 3 x 3 followed by max pooling layer.\n",
        "\n",
        "Stack two more convolutional layers with 128 filters, each of 3 x 3, followed by max pooling, followed by two more convolutional layers with 256 filters, each of 3 x 3, followed by max pooling.\n",
        "\n",
        "Flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
        "\n",
        "*   Change the size of input to 64 x 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm35siILFNT0"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "oH4lDVBuVA_Q",
        "outputId": "8fe2dd04-5065-42e8-8878-47ce8cbed4da"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Resizing\n",
        "\n",
        "# Clear any previous models from memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Define a VGG-like model for CIFAR-10 with input size 64x64\n",
        "model_vgg = Sequential([\n",
        "    # Resize input to 64x64\n",
        "    Resizing(64, 64),\n",
        "\n",
        "    # First two convolutional layers with 64 filters (3x3)\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(64,64,3)),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Next two convolutional layers with 128 filters (3x3)\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Final two convolutional layers with 256 filters (3x3)\n",
        "    Conv2D(256, (3,3), activation='relu', padding='same'),\n",
        "    Conv2D(256, (3,3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Flatten the matrix into a vector\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully connected layer with 128 units\n",
        "    Dense(128, activation='relu'),\n",
        "\n",
        "    # Output layer with 10 classes\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_vgg.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_vgg.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu_B8kJGWhcM"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 10 epochs with a batch size of 512.\n",
        "*   Predict the output for the test split and plot the confusion matrix for the new model and comment on the class confusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4elnDWnjEbmO",
        "outputId": "d2965e25-ee0c-4805-e4b0-3fdb33f7ba29"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compile the model using categorical_crossentropy loss, SGD optimizer, and accuracy metric\n",
        "model_vgg.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on CIFAR-10 dataset for 10 epochs with a batch size of 512\n",
        "history_vgg = model_vgg.fit(x_train, y_train_one_hot, epochs=10, batch_size=512, validation_data=(x_test, y_test_one_hot))\n",
        "\n",
        "# Predict the outputs for the test set\n",
        "y_pred = model_vgg.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probability values to class labels\n",
        "y_true = np.argmax(y_test_one_hot, axis=1)  # Extract true class labels\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for VGG Model on CIFAR-10')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dlzFt0SXGDQ"
      },
      "source": [
        "# Understanding deep networks\n",
        "\n",
        "*   What is the use of activation functions in network? Why is it needed?\n",
        "*   We have used softmax activation function in the exercise. There are other activation functions available too. What is the difference between sigmoid activation and softmax activation?\n",
        "*   What is the difference between categorical crossentropy and binary crossentropy loss?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPy_1EWXX6fp"
      },
      "source": [
        "**Write the answers below :**\n",
        "\n",
        "1 - Use of activation functions:\n",
        "\n",
        "\n",
        "\n",
        "_ Activation functions introduce non-linearity to the network, allowing it to learn complex patterns. Without activation functions, neural networks would behave like simple linear regression models and fail to capture intricate relationships in the data. They help decide which neurons should be activated and propagate meaningful signals through the network.\n",
        "\n",
        "\n",
        "2 - Key Differences between sigmoid and softmax:\n",
        "  - **Sigmoid Activation Function:**\n",
        "     - Outputs values between 0 and 1.\n",
        "     - Used for **binary classification**.\n",
        "     - Can be applied to each neuron independently.\n",
        "   - **Softmax Activation Function:**\n",
        "     - Outputs a probability distribution across multiple classes (values sum to 1).\n",
        "     - Used for **multi-class classification**.\n",
        "     - Helps in selecting the most likely class.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3 - Key Differences between categorical crossentropy and binary crossentropy loss:\n",
        " - **Binary Crossentropy:**\n",
        "     - Used when there are **only two classes** (e.g., yes/no, 0/1).\n",
        "     - Applies a **sigmoid** activation function.\n",
        "   - **Categorical Crossentropy:**\n",
        "     - Used when there are **multiple classes** (e.g., CIFAR-10 with 10 classes).\n",
        "     - Works with **softmax** activation to assign probabilities to different categories.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
